{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational AutoEncoder - Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.datasets as dset\n",
    "from mnist import MNIST # <= 다른거\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNIST('../image_data/MNIST/MNIST/raw')\n",
    "x_train, y_train = mnist.load_training()\n",
    "x_test, y_test = mnist.load_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train 의 length = 60000\n",
      "x_test 의 length = 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train 의 length = {}\".format(len(x_train)))\n",
    "print(\"x_test 의 length = {}\".format(len(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.asarray(x_train)\n",
    "y_train=np.asarray(y_train)\n",
    "x_test=np.asarray(x_test)\n",
    "y_test=np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale the image data to 0 ~ 1. \n",
    "x_train = \n",
    "x_test =\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset using DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# batch size 만큼 나누어서 data를 로드\n",
    "train_loader = torch.utils.data.DataLoader(x_train ,batch_size=batch_size, shuffle=True, num_workers=0,drop_last=False)\n",
    "test_loader = torch.utils.data.DataLoader(x_test ,batch_size=len(x_test), shuffle=False, num_workers=0,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch_idx, image in enumerate(train_loader):\n",
    "    plt.imshow(image[0].reshape(28,28), cmap=\"gray\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make VAE Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        \n",
    "    # ===== 1 ============================================================================================================== #\n",
    "    # Encoder 부분\n",
    "    # hidden layer 를 한번만 통과 한 후 mu 와 variance 의 2개의 output layer 를 return 한다\n",
    "    \n",
    "    def encode(self, x):\n",
    "\n",
    "        \n",
    "        \n",
    "         # Code\n",
    "        \n",
    "        \n",
    "\n",
    "    # ===================================================================================================================== #\n",
    "    \n",
    "    \n",
    "    # ===== 2 ============================================================================================================== #\n",
    "    # Reparameterization Trick\n",
    "    def reparameterize(self, mu_vector, logvar_vector):\n",
    "        # 1) log 를 치워주자!\n",
    "        # 2) 표준편차를 구하자\n",
    "        # 3) Epsilon 을 만들어주자\n",
    "        # 4) reparameterization trick 을 완성하자\n",
    "        \n",
    "        \n",
    "         # Code\n",
    "        \n",
    "        \n",
    "\n",
    "    # ===================================================================================================================== #\n",
    "\n",
    "\n",
    "    # ===== 3 ============================================================================================================== #        \n",
    "    # Decoder 부분\n",
    "    # hidden layer(fc2)를 하나 통과한 후 다시 784 node의 output layer 를 만든후 sigmoid function 을 통과시켜준다.\n",
    "    def decode(self, z):\n",
    "\n",
    "        \n",
    "        # Code\n",
    "        \n",
    "        \n",
    "        \n",
    "    # ===================================================================================================================== #\n",
    "    \n",
    "    \n",
    "    # ===== 4 ============================================================================================================== #\n",
    "    # 위에서부터 정의한 method 들을 이용해 하나의 network 생성\n",
    "    # forward() 메소드는 model 오브젝트를 데이터와 함께 호출하면 자동으로 실행\n",
    "    def forward(self, x):\n",
    "\n",
    "        \n",
    "        \n",
    "         # Code\n",
    "        \n",
    "        \n",
    "        \n",
    "    # ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "\n",
    "\n",
    "        \n",
    "    return Bernouhlli_loss + KL_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/100 epoch\tLoss = 155.71\n",
      "20/100 epoch\tLoss = 150.47\n",
      "30/100 epoch\tLoss = 148.02\n",
      "40/100 epoch\tLoss = 146.54\n",
      "50/100 epoch\tLoss = 145.40\n",
      "60/100 epoch\tLoss = 144.55\n",
      "70/100 epoch\tLoss = 143.89\n",
      "80/100 epoch\tLoss = 143.33\n",
      "90/100 epoch\tLoss = 142.90\n",
      "100/100 epoch\tLoss = 142.40\n"
     ]
    }
   ],
   "source": [
    "for num_epoch in range(epoch):\n",
    "    avg_loss = 0\n",
    "    \n",
    "    for batch_idx, image in enumerate(train_loader):\n",
    "        num_of_mini_batch = len(train_loader) # 235 가 나올것이다 60000/256 = 234.xx => 235\n",
    "        \n",
    "        input_x = image.reshape(-1, 784).to(device)  \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_x, mu, logvar = model(input_x)\n",
    "        \n",
    "        loss = loss_function(recon_x, input_x, mu, logvar)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss += (loss.item()/num_of_mini_batch)\n",
    "        \n",
    "    if (num_epoch+1) % 10 == 0:\n",
    "        print(\"Epoch = {} loss = {:.6f}\".format(num_epoch+1, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './pre_trained/day06_06_VAE_Basic_MNIST_hidden_2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./pre_trained/day06_06_VAE_Basic_MNIST_hidden_2.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, image in enumerate(test_loader):\n",
    "    with torch.no_grad():\n",
    "        test_img = image.to(device)\n",
    "        \n",
    "        test_recon_x, _, _ = model(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10  # 몇개를 보일것인가\n",
    "plt.figure(figsize=(20, 5)) # 전체 figure'들'의 총 행, 열의 크기)\n",
    "\n",
    "for i in range(n):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate New Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    with torch.no_grad():\n",
    "        sample_z = torch.randn(64,2).to(device)\n",
    "        input_z = sample_z\n",
    "        sample = model.decode(input_z).cpu()\n",
    "        save_image(sample.reshape(64, 1, 28, 28),\n",
    "                   './new_images/Basic_VAE_img_' + str(i) + str('_hidden2')+ '.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
