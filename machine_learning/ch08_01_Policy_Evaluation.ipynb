{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P 245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리에겐 MDP Model이 있다. reward & state transition을 알고 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from lib.envs.gridworld import GridworldEnv # 상위 폴더내의 lib 폴더 -> env 폴더 -> gridworld.py 파일에서 GridwordlEnv 를 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridworldEnv() # 강화학습을 위한 환경을 env 변수에 넣음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * 명령어\n",
    "    1. env.nS ☞ number of states in the environment. => 16\n",
    "    2. env.nA ☞ number of actions in the environment. => 4\n",
    "    3. env.P[s] ☞ 각 state 에서 모든 action 에 대한 (trainsition prob, next_state, reward, done) 에 대한 정보를 tuple 로 담고있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 직접 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env 의 총 space 갯수: 16\n",
      "action 의 갯수: 4\n"
     ]
    }
   ],
   "source": [
    "# env 의 space 는 총 16개 이다.\n",
    "print(\"env 의 총 space 갯수:\", env.nS)\n",
    "\n",
    "# 수행 가능한 action 수는 4개이다 (상 하 좌 우)\n",
    "print(\"action 의 갯수:\", env.nA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    또한 숫자에 대응하는 action 을 살펴보면  \n",
    "    UP = 0\n",
    "    RIGHT = 1\n",
    "    DOWN = 2\n",
    "    LEFT = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 위에서 보았듯이,\n",
    "env.P[s] ☞ 각 state 에서 모든 action 에 대한 (trainsition prob, next_state, reward, done) 에 대한 정보를 tuple 로 담고있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "env.P는 각 state마다 액션(action), 확률(probability), 다음 상태(next state), 보상(reward), 종료(done)<br>\n",
    "{action: [(probability, nextstate, reward, done)]} 형식으로 정의 되어 있다.  \n",
    "\n",
    "예를 들어 1번 state 를 살펴보면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 1, -1.0, False)],\n",
       " 1: [(1.0, 2, -1.0, False)],\n",
       " 2: [(1.0, 5, -1.0, False)],\n",
       " 3: [(1.0, 0, -1.0, True)]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉,  \n",
    "1번 state 에서 위쪽 (0) 으로 이동하면  \n",
    "100%의 확률로 1번 state로 transition 하고 (결국 제자리), 보상은 -1 이며 게임을 계속한다 (False)  \n",
    "1번 state 에서 오른쪽 (1) 으로 이동하면  \n",
    "100%의 확률로 2번 state로 transition 하고, 보상은 -1 이며 게임을 계속한다 (False)  \n",
    "1번 state 에서 밑 (2) 으로 이동하면  \n",
    "100%의 확률로 5번 state로 transition 하고, 보상은 -1 이며 게임을 계속한다 (False)  \n",
    "1번 state 에서 왼쪽 (3) 으로 이동하면  \n",
    "100%의 확률로 0번 state로 transition 하고, 보상은 -1 이며 게임은 종료된다 (True)  \n",
    "<br>\n",
    "이는 0번 state 가 terminal state 이기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 1, -1.0, False)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state 1에서 0 action(up)\n",
    "env.P[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "theta : k가 2일때랑 3일때랑 theta보다 덜 차이가 나면 stop하겠다는 것. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(env.nS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "random_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.25\n",
      "1 0.25\n",
      "2 0.25\n",
      "3 0.25\n"
     ]
    }
   ],
   "source": [
    "for action, action_prob in enumerate(random_policy[1]):\n",
    "    print(action, action_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 0, 0.0, True)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(random_policy, env, discount_factor=1.0, theta=0.00001): # theta 는 interation 을 멈추는 시기를 알려주는 역치\n",
    "    # 각 state 의 value 값을 0으로 초기화\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    while True:\n",
    "        # 이전 iteration 에 비해 State Value 값이 얼마나 값이 변했는지 확인하기 위해 설정\n",
    "        delta = 0  \n",
    "        \n",
    "        for state in range(env.nS):\n",
    "            # env.nS = 16 이므로 state 의 값 0~15 까지 => 모든 state 를 한 번씩 돌게된다\n",
    "            v = 0 # 초기화\n",
    "            \n",
    "            # random uniform policy (0.25로 통일) 를 action 별로 action probability 와 함께 가지고 온다\n",
    "            for action, action_prob in enumerate(random_policy[state]):\n",
    "                \n",
    "                # 반드시 기억해야 할 것, \n",
    "                # env.P는 {action: [(probability, nextstate, reward, done)]} 형식으로 정의 되어 있다.\n",
    "                # env.P[state][action] = 특정 state 에서 특정 actiond에 대한 P, S', R, Done 의 값\n",
    "                # V[next_state] = 다음 state 의 value\n",
    "                \n",
    "                # for ~~~~ in env.P[state][action]: 이므로,\n",
    "                # 현재 k 에 대해 특정 state 에 대해서 모든 action 별로 하나씩 value 값을 업데이트 한다\n",
    "                for  transition_prob, next_state, reward, done in env.P[state][action]:\n",
    "                    # Value 값 계산, 교재 246 페이지 참고\n",
    "                    \n",
    "                    # for 문을 이용해서 각 state 마다 서로 다른 action 값에 따른 next state 의 value 값을\n",
    "                    # 계속 더해주기 위해 (Sigma 를 구현하기 위해) += 을 사용해주도록 한다 \n",
    "                    \n",
    "                    v += action_prob * (reward + discount_factor * transition_prob * V[next_state])\n",
    "                    \n",
    "            # 이전 iteration 에 비해 State Value 값이 얼마나 값이 변했는지 확인 = delta\n",
    "            delta = max(delta, np.abs(v - V[state]))\n",
    "            # State 의 value 값 update\n",
    "            V[state] = v\n",
    "        # value 의 변화량이 theta (정해준 역치)보다 적으면 break \n",
    "        if delta < theta:\n",
    "            break\n",
    "            \n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_policy 의 shape\n",
      "(16, 4)\n",
      "16개의 state 에 uniform random policy 가 할당 되어 있다\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "# policy 는 0.25 로 모두 초기화 ()\n",
    "# 윗줄부터 차례대로 State 0~15 에 대한 Policy를 나타낸다\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "print(\"random_policy 의 shape\")\n",
    "print(random_policy.shape)\n",
    "print(\"16개의 state 에 uniform random policy 가 할당 되어 있다\")\n",
    "print(random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Evaluation 수행\n",
    "v = policy_eval(random_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        , -13.99993529, -19.99990698, -21.99989761,\n",
       "       -13.99993529, -17.9999206 , -19.99991379, -19.99991477,\n",
       "       -19.99990698, -19.99991379, -17.99992725, -13.99994569,\n",
       "       -21.99989761, -19.99991477, -13.99994569,   0.        ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function:\n",
      "[  0.         -13.99993529 -19.99990698 -21.99989761 -13.99993529\n",
      " -17.9999206  -19.99991379 -19.99991477 -19.99990698 -19.99991379\n",
      " -17.99992725 -13.99994569 -21.99989761 -19.99991477 -13.99994569\n",
      "   0.        ]\n",
      "\n",
      "Value Function reshape:\n",
      "[[  0.         -13.99993529 -19.99990698 -21.99989761]\n",
      " [-13.99993529 -17.9999206  -19.99991379 -19.99991477]\n",
      " [-19.99990698 -19.99991379 -17.99992725 -13.99994569]\n",
      " [-21.99989761 -19.99991477 -13.99994569   0.        ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function reshape:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
