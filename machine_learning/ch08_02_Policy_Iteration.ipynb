{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from lib.envs.gridworld import GridworldEnv # 상위 폴더내의 lib 폴더 -> env 폴더 -> gridworld.py 파일에서 GridwordlEnv 를 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridworldEnv() # 강화학습을 위한 환경을 env 변수에 넣음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(random_policy, env, discount_factor=1.0, theta=0.00001): # theta 는 interation 을 멈추는 시기를 알려주는 역치\n",
    "    # 각 state 의 value 값을 0으로 초기화\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    while True:\n",
    "        # 이전 iteration 에 비해 State Value 값이 얼마나 값이 변했는지 확인하기 위해 설정\n",
    "        delta = 0  \n",
    "        \n",
    "        for state in range(env.nS):\n",
    "            # env.nS = 16 이므로 state 의 값 0~15 까지 => 모든 state 를 한 번씩 돌게된다\n",
    "            v = 0 # 초기화\n",
    "            \n",
    "            # random uniform policy (0.25로 통일) 를 action 별로 action probability 와 함께 가지고 온다\n",
    "            for action, action_prob in enumerate(random_policy[state]):\n",
    "                \n",
    "                # 반드시 기억해야 할 것, \n",
    "                # env.P는 {action: [(probability, nextstate, reward, done)]} 형식으로 정의 되어 있다.\n",
    "                # env.P[state][action] = 특정 state 에서 특정 actiond에 대한 P, S', R, Done 의 값\n",
    "                # V[next_state] = 다음 state 의 value\n",
    "                \n",
    "                # for ~~~~ in env.P[state][action]: 이므로,\n",
    "                # 현재 k 에 대해 특정 state 에 대해서 모든 action 별로 하나씩 value 값을 업데이트 한다\n",
    "                for  transition_prob, next_state, reward, done in env.P[state][action]:\n",
    "                    # Value 값 계산, 교재 246 페이지 참고\n",
    "                    \n",
    "                    # for 문을 이용해서 각 state 마다 서로 다른 action 값에 따른 next state 의 value 값을\n",
    "                    # 계속 더해주기 위해 (Sigma 를 구현하기 위해) += 을 사용해주도록 한다 \n",
    "                    \n",
    "                    v += action_prob * (reward + discount_factor * transition_prob * V[next_state])\n",
    "            \n",
    "            # 이전 iteration 에 비해 State Value 값이 얼마나 값이 변했는지 확인 = delta\n",
    "            delta = max(delta, np.abs(v - V[state]))\n",
    "            # State 의 value 값 update\n",
    "            V[state] = v\n",
    "        # value 의 변화량이 theta (정해준 역치)보다 적으면 break \n",
    "        if delta < theta:\n",
    "            break\n",
    "            \n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policy_eval 함수는 state돌면서, `state value function`을 새로운 값으로 업데이트 해주는 함수. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imporovement Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.nA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(env.nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(env.nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \n",
    "    # Q-value (action_value) 계산하는 함수 정의 \n",
    "    def one_step_lookahead(state, State_Value): \n",
    "        # 이 함수는, state랑 state_value(모든 state 값 통째로) 받아서 해당 state의 각 액션별 Q값을 계산해서 리턴해 준다. \n",
    "        action_value = np.zeros(env.nA) # 초기화 시켜줌 : array([0., 0., 0., 0.]) & shape = (4,)\n",
    "        \n",
    "        for action in range(env.nA): # 각 action 별로 value 구하기 (후에 state 와 같이 for문을 돌게 된다)\n",
    "            \n",
    "            for transition_prob, next_state, reward, done in env.P[state][action]: # next state 에서 value 값을 구하기위해\n",
    "                # 교재 240 page 공식 참조\n",
    "                # State_Value[next_state] 는 Next state 에서 V pi 를 나타낸다\n",
    "                # State_Value 가 Evaluate 된 Value 이므로 가장 좋은 policy를 나타내게 될 것이다\n",
    "                action_value[action] += reward + (discount_factor * transition_prob * State_Value[next_state])\n",
    "                \n",
    "                # print(\"action_value (Q-value):\",action_value)\n",
    "                \n",
    "        return action_value\n",
    "    \n",
    "    # Start with a random policy\n",
    "    # 최초의 policy 는 state 마다 0.25 로 모두 초기화 (동 서 남 북 전부 0.25의 확률로 움직인다)\n",
    "    \n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    '''\n",
    "    policy ↓\n",
    "     ([[0.25, 0.25, 0.25, 0.25],\n",
    "       [0.25, 0.25, 0.25, 0.25],\n",
    "       [0.25, 0.25, 0.25, 0.25],\n",
    "       [0.25, 0.25, 0.25, 0.25],\n",
    "       [0.25, 0.25, 0.25, 0.25],\n",
    "       [0.25, 0.25, 0.25, 0.25],\n",
    "       [0.25, 0.25, 0.25, 0.25],\n",
    "       [0.25, 0.25, 0.25, 0.25],\n",
    "       [0.25, 0.25, 0.25, 0.25],\n",
    "       [0.25, 0.25, 0.25, 0.25],\n",
    "       [0.25, 0.25, 0.25, 0.25],\n",
    "       [0.25, 0.25, 0.25, 0.25],\n",
    "       [0.25, 0.25, 0.25, 0.25],\n",
    "       [0.25, 0.25, 0.25, 0.25],\n",
    "       [0.25, 0.25, 0.25, 0.25],\n",
    "       [0.25, 0.25, 0.25, 0.25]])'''\n",
    "    \n",
    "    # improvement 가 한 번 끝난 이후에는 policy 를 improve 한다\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # 1. 가장먼저 해야 할 일 : 현재 policy 를 evaluation        \n",
    "        state_value = policy_eval_fn(policy, env, discount_factor) # <- policy evaluation code\n",
    "        print(\"==============================================\")\n",
    "        print(\"State_value\", )\n",
    "        print(state_value.reshape([4,4]))\n",
    "\n",
    "        # 각 State 별로 policy 가 evaluation 되었다 (state 별로 return 을 얼마 받을지 예측)\n",
    "        \n",
    "        # 2. 이제부터는 improvement 를 해야한다\n",
    "        \n",
    "        # while 문 종료 시점\n",
    "        policy_stable = True\n",
    "        \n",
    "        # 각각의 state 에 대하여 계산\n",
    "        for state in range(env.nS): # state 0부터 15까지 계산\n",
    "            print(\"\\n\")\n",
    "            print(\"==============================================\")\n",
    "            print(\"시작 state =\",state)\n",
    "            \n",
    "            # policy[state] 이므로 state 별로 돌아가며 최적의 policy 를 구한다\n",
    "            # argmax 함수를 이용해서 최적의 policy 를 구하도록 한다\n",
    "            # argmax 는 max 값을 가지고 있는 원소의 'INDEX' 를 반환한다 [0,0,1,0] 이라면 2 를 반환\n",
    "            # 즉 chosen_action 에는 해당 state 의 policy 중 가장 높은 값을 가지고 있는 action의 인덱스를 반환시켜주는 것이다\n",
    "            \n",
    "            # state 마다 할당되어 있는 policy 를 argmax 해서 가장 알맞은 action 값을 가지고 온다\n",
    "            # 하지만 처음의 policy 는 random 한 policy 이므로 옳지 않은 policy 가 저장되어 있을 것이다\n",
    "            # 이후에 나오는 policy_eval 를 이용하여 value 값을 다시 구한 뒤, policy 를 구해본다.\n",
    "            chosen_action = np.argmax(policy[state])\n",
    "            print(\"chosen_action\", chosen_action ,\"\\n\")\n",
    "            print(\"before improve policy\", policy[state])\n",
    "            \n",
    "            # policy_eval 함수를 이용해 state 별로 action 의 value 값을 계산\n",
    "            # state 1 에서 action 0 의 value 는 몇인지, action 1 의 value 는 몇인지 등등을 쭉 구한다\n",
    "            action_values = one_step_lookahead(state, state_value)\n",
    "            print(\"state {} 에서의 최종 action value (Q-value) 값:{}\".format(state,action_values), \"\\n\")\n",
    "            # action_value 가 return 되었다\n",
    "            \n",
    "            # 계산한 것을 토대로 그때그때의 best action 을 찾는다\n",
    "            best_action = np.argmax(action_values)\n",
    "            print(\"가장 큰 value 값:\", action_values[best_action])\n",
    "            print(\"best_action\",best_action)\n",
    "            \n",
    "            # Greedily update the policy\n",
    "            # 두개가 같다면 True 값을 반환\n",
    "            if chosen_action != best_action:\n",
    "                policy_stable = False\n",
    "                print(\"기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 다르다 \\n\")\n",
    "            elif chosen_action == best_action:\n",
    "                print(\"기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \\n\")\n",
    "            \n",
    "            policy[state] = np.eye(env.nA)[best_action] # np.eye = 단위행렬 생성, best action 하나를 선택해서 index 로 주면\n",
    "                                                        # 그에 해당하는 policy 가 만들어진다\n",
    "                                                        # 예를들어 array([1, 0, 0, 0]) 이런식으로 만들어진다\n",
    "                                                        # 이러한 코드를 이용해서 해당 state 에 맞는 policy 를 주도록한다.\n",
    "            '''\n",
    "            np.eye(env.nA) ↓\n",
    "           ([[1., 0., 0., 0.],\n",
    "             [0., 1., 0., 0.],\n",
    "             [0., 0., 1., 0.],\n",
    "             [0., 0., 0., 1.]]) 여기서 index 를 1 로 주면 [0, 1, 0, 0] 이 튀어 나올 것\n",
    "             '''\n",
    "            print(\"after improve policy\", policy[state])\n",
    "        print(\"\")\n",
    "        \n",
    "        # State 별로 다 돌면 policy_state 를 검사한다\n",
    "        # If the policy is stable we've found an optimal policy. Return it\n",
    "        # chosen_action 와 best_action 이 같지 않다면 False 이므로 반환이 되지 않으므로 while 문이 종료되지 않는다\n",
    "        # chosen_action 와 best_action 이 같다면 True 가 되고 return 값이 생기므로 while 문이 종료된다\n",
    "        \n",
    "        # ☆모든 state 에 대해서 전부 같아야한다☆\n",
    "        print(\"policy_stable?\", policy_stable)\n",
    "        if policy_stable:\n",
    "            print(\"Policy 가 stable 하다\")\n",
    "            return policy, state_value\n",
    "        else:\n",
    "            print(\"Policy 가 stable 하지 않다\")\n",
    "            print(\"==============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "State_value\n",
      "[[  0.         -13.99993529 -19.99990698 -21.99989761]\n",
      " [-13.99993529 -17.9999206  -19.99991379 -19.99991477]\n",
      " [-19.99990698 -19.99991379 -17.99992725 -13.99994569]\n",
      " [-21.99989761 -19.99991477 -13.99994569   0.        ]]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 0\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [0.25 0.25 0.25 0.25]\n",
      "state 0 에서의 최종 action value (Q-value) 값:[0. 0. 0. 0.] \n",
      "\n",
      "가장 큰 value 값: 0.0\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 1\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [0.25 0.25 0.25 0.25]\n",
      "state 1 에서의 최종 action value (Q-value) 값:[-14.99993529 -20.99990698 -18.9999206   -1.        ] \n",
      "\n",
      "가장 큰 value 값: -1.0\n",
      "best_action 3\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 다르다 \n",
      "\n",
      "after improve policy [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 2\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [0.25 0.25 0.25 0.25]\n",
      "state 2 에서의 최종 action value (Q-value) 값:[-20.99990698 -22.99989761 -20.99991379 -14.99993529] \n",
      "\n",
      "가장 큰 value 값: -14.99993529374931\n",
      "best_action 3\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 다르다 \n",
      "\n",
      "after improve policy [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 3\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [0.25 0.25 0.25 0.25]\n",
      "state 3 에서의 최종 action value (Q-value) 값:[-22.99989761 -22.99989761 -20.99991477 -20.99990698] \n",
      "\n",
      "가장 큰 value 값: -20.999906976106587\n",
      "best_action 3\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 다르다 \n",
      "\n",
      "after improve policy [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 4\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [0.25 0.25 0.25 0.25]\n",
      "state 4 에서의 최종 action value (Q-value) 값:[ -1.         -18.9999206  -20.99990698 -14.99993529] \n",
      "\n",
      "가장 큰 value 값: -1.0\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 5\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [0.25 0.25 0.25 0.25]\n",
      "state 5 에서의 최종 action value (Q-value) 값:[-14.99993529 -20.99991379 -20.99991379 -14.99993529] \n",
      "\n",
      "가장 큰 value 값: -14.99993529374931\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 6\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [0.25 0.25 0.25 0.25]\n",
      "state 6 에서의 최종 action value (Q-value) 값:[-20.99990698 -20.99991477 -18.99992725 -18.9999206 ] \n",
      "\n",
      "가장 큰 value 값: -18.999920596940385\n",
      "best_action 3\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 다르다 \n",
      "\n",
      "after improve policy [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 7\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [0.25 0.25 0.25 0.25]\n",
      "state 7 에서의 최종 action value (Q-value) 값:[-22.99989761 -20.99991477 -14.99994569 -20.99991379] \n",
      "\n",
      "가장 큰 value 값: -14.99994568515821\n",
      "best_action 2\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 다르다 \n",
      "\n",
      "after improve policy [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 8\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [0.25 0.25 0.25 0.25]\n",
      "state 8 에서의 최종 action value (Q-value) 값:[-14.99993529 -20.99991379 -22.99989761 -20.99990698] \n",
      "\n",
      "가장 큰 value 값: -14.99993529374931\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 9\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [0.25 0.25 0.25 0.25]\n",
      "state 9 에서의 최종 action value (Q-value) 값:[-18.9999206  -18.99992725 -20.99991477 -20.99990698] \n",
      "\n",
      "가장 큰 value 값: -18.999920596940385\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 10\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [0.25 0.25 0.25 0.25]\n",
      "state 10 에서의 최종 action value (Q-value) 값:[-20.99991379 -14.99994569 -14.99994569 -20.99991379] \n",
      "\n",
      "가장 큰 value 값: -14.99994568515821\n",
      "best_action 1\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 다르다 \n",
      "\n",
      "after improve policy [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 11\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [0.25 0.25 0.25 0.25]\n",
      "state 11 에서의 최종 action value (Q-value) 값:[-20.99991477 -14.99994569  -1.         -18.99992725] \n",
      "\n",
      "가장 큰 value 값: -1.0\n",
      "best_action 2\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 다르다 \n",
      "\n",
      "after improve policy [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 12\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [0.25 0.25 0.25 0.25]\n",
      "state 12 에서의 최종 action value (Q-value) 값:[-20.99990698 -20.99991477 -22.99989761 -22.99989761] \n",
      "\n",
      "가장 큰 value 값: -20.999906976106587\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 13\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [0.25 0.25 0.25 0.25]\n",
      "state 13 에서의 최종 action value (Q-value) 값:[-20.99991379 -14.99994569 -20.99991477 -22.99989761] \n",
      "\n",
      "가장 큰 value 값: -14.999945685158211\n",
      "best_action 1\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 다르다 \n",
      "\n",
      "after improve policy [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 14\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [0.25 0.25 0.25 0.25]\n",
      "state 14 에서의 최종 action value (Q-value) 값:[-18.99992725  -1.         -14.99994569 -20.99991477] \n",
      "\n",
      "가장 큰 value 값: -1.0\n",
      "best_action 1\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 다르다 \n",
      "\n",
      "after improve policy [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 15\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [0.25 0.25 0.25 0.25]\n",
      "state 15 에서의 최종 action value (Q-value) 값:[0. 0. 0. 0.] \n",
      "\n",
      "가장 큰 value 값: 0.0\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "policy_stable? False\n",
      "Policy 가 stable 하지 않다\n",
      "==============================================\n",
      "==============================================\n",
      "State_value\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 0\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [1. 0. 0. 0.]\n",
      "state 0 에서의 최종 action value (Q-value) 값:[0. 0. 0. 0.] \n",
      "\n",
      "가장 큰 value 값: 0.0\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 1\n",
      "chosen_action 3 \n",
      "\n",
      "before improve policy [0. 0. 0. 1.]\n",
      "state 1 에서의 최종 action value (Q-value) 값:[-2. -3. -3. -1.] \n",
      "\n",
      "가장 큰 value 값: -1.0\n",
      "best_action 3\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 2\n",
      "chosen_action 3 \n",
      "\n",
      "before improve policy [0. 0. 0. 1.]\n",
      "state 2 에서의 최종 action value (Q-value) 값:[-3. -4. -4. -2.] \n",
      "\n",
      "가장 큰 value 값: -2.0\n",
      "best_action 3\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 3\n",
      "chosen_action 3 \n",
      "\n",
      "before improve policy [0. 0. 0. 1.]\n",
      "state 3 에서의 최종 action value (Q-value) 값:[-4. -4. -3. -3.] \n",
      "\n",
      "가장 큰 value 값: -3.0\n",
      "best_action 2\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 다르다 \n",
      "\n",
      "after improve policy [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 4\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [1. 0. 0. 0.]\n",
      "state 4 에서의 최종 action value (Q-value) 값:[-1. -3. -3. -2.] \n",
      "\n",
      "가장 큰 value 값: -1.0\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 5\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [1. 0. 0. 0.]\n",
      "state 5 에서의 최종 action value (Q-value) 값:[-2. -4. -4. -2.] \n",
      "\n",
      "가장 큰 value 값: -2.0\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 6\n",
      "chosen_action 3 \n",
      "\n",
      "before improve policy [0. 0. 0. 1.]\n",
      "state 6 에서의 최종 action value (Q-value) 값:[-3. -3. -3. -3.] \n",
      "\n",
      "가장 큰 value 값: -3.0\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 다르다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 7\n",
      "chosen_action 2 \n",
      "\n",
      "before improve policy [0. 0. 1. 0.]\n",
      "state 7 에서의 최종 action value (Q-value) 값:[-4. -3. -2. -4.] \n",
      "\n",
      "가장 큰 value 값: -2.0\n",
      "best_action 2\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 8\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [1. 0. 0. 0.]\n",
      "state 8 에서의 최종 action value (Q-value) 값:[-2. -4. -4. -3.] \n",
      "\n",
      "가장 큰 value 값: -2.0\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 9\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [1. 0. 0. 0.]\n",
      "state 9 에서의 최종 action value (Q-value) 값:[-3. -3. -3. -3.] \n",
      "\n",
      "가장 큰 value 값: -3.0\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 10\n",
      "chosen_action 1 \n",
      "\n",
      "before improve policy [0. 1. 0. 0.]\n",
      "state 10 에서의 최종 action value (Q-value) 값:[-4. -2. -2. -4.] \n",
      "\n",
      "가장 큰 value 값: -2.0\n",
      "best_action 1\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 11\n",
      "chosen_action 2 \n",
      "\n",
      "before improve policy [0. 0. 1. 0.]\n",
      "state 11 에서의 최종 action value (Q-value) 값:[-3. -2. -1. -3.] \n",
      "\n",
      "가장 큰 value 값: -1.0\n",
      "best_action 2\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 12\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [1. 0. 0. 0.]\n",
      "state 12 에서의 최종 action value (Q-value) 값:[-3. -3. -4. -4.] \n",
      "\n",
      "가장 큰 value 값: -3.0\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 13\n",
      "chosen_action 1 \n",
      "\n",
      "before improve policy [0. 1. 0. 0.]\n",
      "state 13 에서의 최종 action value (Q-value) 값:[-4. -2. -3. -4.] \n",
      "\n",
      "가장 큰 value 값: -2.0\n",
      "best_action 1\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 14\n",
      "chosen_action 1 \n",
      "\n",
      "before improve policy [0. 1. 0. 0.]\n",
      "state 14 에서의 최종 action value (Q-value) 값:[-3. -1. -2. -3.] \n",
      "\n",
      "가장 큰 value 값: -1.0\n",
      "best_action 1\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 15\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [1. 0. 0. 0.]\n",
      "state 15 에서의 최종 action value (Q-value) 값:[0. 0. 0. 0.] \n",
      "\n",
      "가장 큰 value 값: 0.0\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "policy_stable? False\n",
      "Policy 가 stable 하지 않다\n",
      "==============================================\n",
      "==============================================\n",
      "State_value\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 0\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [1. 0. 0. 0.]\n",
      "state 0 에서의 최종 action value (Q-value) 값:[0. 0. 0. 0.] \n",
      "\n",
      "가장 큰 value 값: 0.0\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 1\n",
      "chosen_action 3 \n",
      "\n",
      "before improve policy [0. 0. 0. 1.]\n",
      "state 1 에서의 최종 action value (Q-value) 값:[-2. -3. -3. -1.] \n",
      "\n",
      "가장 큰 value 값: -1.0\n",
      "best_action 3\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 2\n",
      "chosen_action 3 \n",
      "\n",
      "before improve policy [0. 0. 0. 1.]\n",
      "state 2 에서의 최종 action value (Q-value) 값:[-3. -4. -4. -2.] \n",
      "\n",
      "가장 큰 value 값: -2.0\n",
      "best_action 3\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [0. 0. 0. 1.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 3\n",
      "chosen_action 2 \n",
      "\n",
      "before improve policy [0. 0. 1. 0.]\n",
      "state 3 에서의 최종 action value (Q-value) 값:[-4. -4. -3. -3.] \n",
      "\n",
      "가장 큰 value 값: -3.0\n",
      "best_action 2\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 4\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [1. 0. 0. 0.]\n",
      "state 4 에서의 최종 action value (Q-value) 값:[-1. -3. -3. -2.] \n",
      "\n",
      "가장 큰 value 값: -1.0\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 5\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [1. 0. 0. 0.]\n",
      "state 5 에서의 최종 action value (Q-value) 값:[-2. -4. -4. -2.] \n",
      "\n",
      "가장 큰 value 값: -2.0\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 6\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [1. 0. 0. 0.]\n",
      "state 6 에서의 최종 action value (Q-value) 값:[-3. -3. -3. -3.] \n",
      "\n",
      "가장 큰 value 값: -3.0\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 7\n",
      "chosen_action 2 \n",
      "\n",
      "before improve policy [0. 0. 1. 0.]\n",
      "state 7 에서의 최종 action value (Q-value) 값:[-4. -3. -2. -4.] \n",
      "\n",
      "가장 큰 value 값: -2.0\n",
      "best_action 2\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 8\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [1. 0. 0. 0.]\n",
      "state 8 에서의 최종 action value (Q-value) 값:[-2. -4. -4. -3.] \n",
      "\n",
      "가장 큰 value 값: -2.0\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 9\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [1. 0. 0. 0.]\n",
      "state 9 에서의 최종 action value (Q-value) 값:[-3. -3. -3. -3.] \n",
      "\n",
      "가장 큰 value 값: -3.0\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 10\n",
      "chosen_action 1 \n",
      "\n",
      "before improve policy [0. 1. 0. 0.]\n",
      "state 10 에서의 최종 action value (Q-value) 값:[-4. -2. -2. -4.] \n",
      "\n",
      "가장 큰 value 값: -2.0\n",
      "best_action 1\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 11\n",
      "chosen_action 2 \n",
      "\n",
      "before improve policy [0. 0. 1. 0.]\n",
      "state 11 에서의 최종 action value (Q-value) 값:[-3. -2. -1. -3.] \n",
      "\n",
      "가장 큰 value 값: -1.0\n",
      "best_action 2\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [0. 0. 1. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 12\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [1. 0. 0. 0.]\n",
      "state 12 에서의 최종 action value (Q-value) 값:[-3. -3. -4. -4.] \n",
      "\n",
      "가장 큰 value 값: -3.0\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 13\n",
      "chosen_action 1 \n",
      "\n",
      "before improve policy [0. 1. 0. 0.]\n",
      "state 13 에서의 최종 action value (Q-value) 값:[-4. -2. -3. -4.] \n",
      "\n",
      "가장 큰 value 값: -2.0\n",
      "best_action 1\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 14\n",
      "chosen_action 1 \n",
      "\n",
      "before improve policy [0. 1. 0. 0.]\n",
      "state 14 에서의 최종 action value (Q-value) 값:[-3. -1. -2. -3.] \n",
      "\n",
      "가장 큰 value 값: -1.0\n",
      "best_action 1\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [0. 1. 0. 0.]\n",
      "\n",
      "\n",
      "==============================================\n",
      "시작 state = 15\n",
      "chosen_action 0 \n",
      "\n",
      "before improve policy [1. 0. 0. 0.]\n",
      "state 15 에서의 최종 action value (Q-value) 값:[0. 0. 0. 0.] \n",
      "\n",
      "가장 큰 value 값: 0.0\n",
      "best_action 0\n",
      "기존의 action (chosen_action) 과 improvement 한 action (best_action) 이 서로 같다 \n",
      "\n",
      "after improve policy [1. 0. 0. 0.]\n",
      "\n",
      "policy_stable? True\n",
      "Policy 가 stable 하다\n"
     ]
    }
   ],
   "source": [
    "policy, v = policy_improvement(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Probability Distribution:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 2]\n",
      " [0 0 0 2]\n",
      " [0 0 1 2]\n",
      " [0 1 1 0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function:\n",
      "[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "# optimal policy 를 따라 갔을때의 value 값\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
